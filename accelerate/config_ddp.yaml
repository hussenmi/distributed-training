# Accelerate Configuration: Distributed Data Parallel (DDP)
#
# DDP replicates the entire model on each GPU and synchronizes gradients
# after each backward pass. Best when the model fits in a single GPU's memory.
#
# Usage: accelerate launch --config_file config_ddp.yaml train.py

compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
enable_cpu_affinity: false
gpu_ids: all
machine_rank: 0
main_training_function: main
mixed_precision: fp16
num_machines: 1
num_processes: 2  # Adjust based on available GPUs
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
